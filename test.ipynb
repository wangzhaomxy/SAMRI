{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Inference of the MRI-SAM on the nifti datasets.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "from utils.parsers import test_parser\n",
    "from utils.visual import *\n",
    "from utils.utils import *\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.dataloader import NiiDataset\n",
    "from utils.prompt import *\n",
    "from tqdm import tqdm\n",
    "from utils.losses import dice_similarity\n",
    "\n",
    "\"\"\"\n",
    "# load args from command line\n",
    "args = test_parser.parse_args()\n",
    "\n",
    "device = args.device\n",
    "checkpoint=args.checkpoint\n",
    "encoder_tpye = args.encoder_tpye\n",
    "\n",
    "# Use args in the formal publics\n",
    "\"\"\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoder_tpye = ENCODER_TYPE\n",
    "checkpoint = SAM_CHECKPOINT\n",
    "device = DEVICE\n",
    "\n",
    "# regist the MRI-SAM model and predictor.\n",
    "mri_sam_model = sam_model_registry[encoder_tpye](checkpoint)\n",
    "mri_sam_model = mri_sam_model.to(device)\n",
    "predictor = SamPredictor(mri_sam_model)\n",
    "\n",
    "# load dataset\n",
    "file_path = IMAGE_PATH\n",
    "test_dataset = NiiDataset(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# setup essential parameters.\n",
    "num_points = NUM_POINTS\n",
    "num_bboxes = NUM_BBOXES\n",
    "jitter = JITTER\n",
    "record = {}\n",
    "save_path = SAVE_PATH\n",
    "\n",
    "for image, mask in tqdm(test_dataset):\n",
    "    # Image embedding inference\n",
    "    predictor.set_image(image)\n",
    "    \n",
    "    name = test_dataset.get_name()\n",
    "    # split the multi-labeled mask into single labeled\n",
    "    # logit masks.\n",
    "    masks = MaskSplit(mask)\n",
    "\n",
    "    sub_record = {\"p\":[], \"b\":[]}\n",
    "    p_rec = []\n",
    "    b_rec = []    \n",
    "    if num_points == num_bboxes == 1:\n",
    "        for each_label_mask in masks: # shape is HW=(255, 255)\n",
    "            # generate prompts\n",
    "            point = gen_points(each_label_mask)\n",
    "            point_lable = np.array([1])\n",
    "            bbox = gen_bboxes(each_label_mask, jitter=jitter)\n",
    "\n",
    "            # generate mask\n",
    "            pre_mask_p, _, _ = predictor.predict(\n",
    "                                point_coords=point,\n",
    "                                point_labels=point_lable,\n",
    "                                multimask_output=False,\n",
    "                            )\n",
    "            \n",
    "            pre_mask_b, _, _ = predictor.predict(\n",
    "                                point_coords=None,\n",
    "                                point_labels=None,\n",
    "                                box=bbox[None, :],\n",
    "                                multimask_output=False,\n",
    "                            )\n",
    "            \n",
    "            p_dice = dice_similarity(each_label_mask, pre_mask_p[0, :, :])\n",
    "            b_dice = dice_similarity(each_label_mask, pre_mask_b[0, :, :])\n",
    "\n",
    "            p_rec.append(p_dice)\n",
    "            b_rec.append(b_dice)\n",
    "        sub_record[\"p\"].append(p_rec)\n",
    "        sub_record[\"b\"].append(b_rec)\n",
    "\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    record[name] = sub_record\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
