Metadata-Version: 2.4
Name: samri
Version: 0.0.1
Summary: SAMRI: Segment Anything Model for MRI â€” decoder-only fine-tuning on precomputed MRI embeddings.
Home-page: https://github.com/zhaowangmxy/SAMRI
Author: Zhao Wang
Author-email: zhao.wang1@uq.edu.au
License: Apache-2.0
Keywords: MRI,segmentation,medical-imaging,deep-learning,SAM
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Science/Research
Classifier: License :: OSI Approved :: Apache Software License
Classifier: Programming Language :: Python :: 3 :: Only
Classifier: Programming Language :: Python :: 3.10
Classifier: Topic :: Scientific/Engineering :: Medical Science Apps.
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: Operating System :: OS Independent
Requires-Python: >=3.10
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: numpy
Requires-Dist: scipy
Requires-Dist: scikit-image
Requires-Dist: nibabel
Requires-Dist: SimpleITK>=2.2.1
Requires-Dist: opencv-python
Requires-Dist: matplotlib
Requires-Dist: tqdm
Requires-Dist: einops
Provides-Extra: coco
Requires-Dist: pycocotools; platform_system != "Windows" and extra == "coco"
Provides-Extra: notebooks
Requires-Dist: jupyterlab; extra == "notebooks"
Requires-Dist: ipympl; extra == "notebooks"
Requires-Dist: ipywidgets; extra == "notebooks"
Provides-Extra: dev
Requires-Dist: flake8; extra == "dev"
Requires-Dist: isort; extra == "dev"
Requires-Dist: black; extra == "dev"
Requires-Dist: mypy; extra == "dev"
Dynamic: author
Dynamic: author-email
Dynamic: classifier
Dynamic: description
Dynamic: description-content-type
Dynamic: home-page
Dynamic: keywords
Dynamic: license
Dynamic: license-file
Dynamic: provides-extra
Dynamic: requires-dist
Dynamic: requires-python
Dynamic: summary

# ğŸ§  SAMRI: Segment Anything Model for MRI

**SAMRI** is an MRI-specialized adaptation of [Meta AIâ€™s Segment Anything Model (SAM)](https://segment-anything.com/), designed for accurate and efficient segmentation across diverse MRI datasets.  
By fine-tuning only the **lightweight mask decoder** on **precomputed MRI embeddings**, SAMRI achieves state-of-the-art Dice and boundary accuracy while drastically reducing computational cost.

---

## ğŸŒŸ Highlights

- ğŸ§© **Decoder-only fine-tuning** â€” freeze SAMâ€™s heavy image encoder and prompt encoder.  
- âš™ï¸ **Two-stage pipeline** â€” precompute embeddings â†’ fine-tune decoder.  
- ğŸ§  **1.1 M MRI pairs** from **36 datasets / 47 tasks** across **10+ MRI protocols**.  
- ğŸš€ **94% shorter training time** and **96% fewer trainable parameters** than full SAM retraining.  
- ğŸ“ˆ **Superior segmentation** on small and medium structures, with strong zero-shot generalization.  
- ğŸ–¼ï¸ Supports **box, point, and box + point prompts**.  

---

![SAMRI Architecture](docs/fig_samri_architecture.png)  
*Figure 1. Overview of SAMRI: frozen image encoder and prompt encoder, lightweight decoder fine-tuning.*

---

## ğŸ§­ Overview

**SAMRI** adapts SAM for the MRI domain by leveraging SAMâ€™s strong visual representations while tailoring the decoder to medical structures and contrasts.  
The approach:
1. **Precomputes embeddings** using SAM ViT-B encoder on 2D MRI slices.  
2. **Fine-tunes only the mask decoder** with a hybrid focalâ€“Dice loss for domain adaptation.  

This lightweight strategy allows SAMRI to train efficiently on a **single GPU** or **multi-GPU clusters** (e.g., H100 x 8), while maintaining robust accuracy across unseen datasets and imaging protocols.

---
## ğŸ› ï¸ Installation (SAMRI)

This section helps you go from **zero to a runnable environment** for SAMRI. It includes optional prerequisites, a reproducible Conda setup, and a brief explanation of how dependency installation works.



### ğŸ§© Step 0 â€” Prerequisites (Optional but recommended)
SAMRI requires **Python â‰¥ 3.10** and **PyTorch â‰¥ 2.2** (CUDA or ROCm recommended).  
Use a package manager like **Conda** to isolate dependencies per project.

- Download [**Anaconda**:arrow_upper_right:](https://www.anaconda.com/download)
- Download [**Miniconda (lightweight)**:arrow_upper_right:](https://docs.conda.io/en/latest/miniconda.html)


Verify Conda is available:
```bash
conda --version
```

### ğŸ” Step 1 â€” Create and activate a fresh environment
If you already have a base environment:

```bash
conda create -n samri python=3.10 -y
conda activate samri
```

### ğŸ§ª Step 2 - Install PyTorch

Please install the correct [PyTorch:arrow_upper_right:](https://pytorch.org) version according to your operating system, package manager, language, and compute platform.
This project has been verified on PyTorch 2.2.0.

### ğŸ§° Step 3 â€” Clone the Repository and install dependencies
```bash
git clone https://github.com/wangzhaomxy/SAMRI.git
cd SAMRI
pip install .
```

### âœ… Step 4 â€” Verify Your Setup
Run a quick import test in the command line:
```bash
python -c "import torch, nibabel; print('SAMRI environment ready! Torch:', torch.__version__)"
```

If it prints without errors, your environment is correctly configured.

---
## ğŸš€ Quick Start

### 1ï¸âƒ£ Precompute Image Embeddings
Generate and save SAM ViT-B image embeddings for your MRI dataset:
```bash
python preprocess/precompute_embeddings.py   --data_dir /path/to/MRI_dataset   --save_dir ./embeddings   --batch_size 16
```

### 2ï¸âƒ£ Train the Mask Decoder
Fine-tune the decoder using precomputed embeddings:
```bash
torchrun --nproc_per_node=8 train_decoder.py   --embedding_dir ./embeddings   --epochs 30   --batch_size 16   --lr 1e-4
```

### 3ï¸âƒ£ Inference and Visualization
Run inference on new MRI slices:
```bash
python infer_results.py   --model_path checkpoints/samri_decoder.pth   --input_image sample.nii.gz   --prompt box
```

The results (mask overlays, metrics, and visual comparisons) are saved under `./Inference_results`.

---

## ğŸ“Š Evaluation

SAMRI is evaluated using:
- **Dice Similarity Coefficient (DSC)**  
- **Hausdorff Distance (HD)**  
- **Mean Surface Distance (MSD)**  

We further group structures by **relative size**:  
- *Small* (< 0.5%), *Medium* (0.5â€“3.5%), *Large* (> 3.5%)  
and apply **Wilcoxon signed-rank tests** to assess significance.

![Results](docs/fig_samri_results.png)  
*Figure 2. Dice comparison between SAM ViT-B, MedSAM, and SAMRI across object-size bins.*

---

## ğŸ§  Dataset Overview

SAMRI is trained on a curated **1.1 million MRI imageâ€“mask pairs** from **36 public datasets** (47 segmentation tasks) spanning over **10 MRI sequences** (T1, T2, FLAIR, DESS, TSE, etc.).

| Category | Example Datasets | Approx. Pairs |
|-----------|------------------|---------------|
| **Brain** | BraTS, ISLES, MSD_Hippocampus | 420 K |
| **Abdomen** | AMOSMR, HipMRI | 260 K |
| **Knee** | MSK_T2, OAI, DESS | 210 K |
| **Thorax** | Heart, MSD_Heart | 130 K |
| **Others** | Prostate, MSK_FLASH | 80 K |

Detailed dataset breakdowns are provided in **Table S1 (Supplementary)**.

---

## ğŸ“‚ Repository Structure

```
SAMRI/
â”œâ”€â”€ configs/             # Dataset/task configurations (YAML)
â”œâ”€â”€ preprocess/          # Precompute embeddings and data utilities
â”œâ”€â”€ train_decoder.py     # Decoder fine-tuning script
â”œâ”€â”€ infer_results.py     # Inference and visualization pipeline
â”œâ”€â”€ utils/               # Metrics, plotting, and helper functions
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ§ª Results Summary

| Model | Trainable Params | Training Time (8Ã— MI300X) | Dice â†‘ | HD â†“ | MSD â†“ |
|--------|------------------|----------------------------|--------|------|------|
| SAM ViT-B (zero-shot) | 0 % | â€” | Baseline | â€” | â€” |
| MedSAM | 100 % | > 600 h | Moderate | â€” | â€” |
| **SAMRI (Ours)** | **4 %** | **76 h** | **â†‘ Dice, â†“ HD, â†“ MSD** | | |

SAMRI shows the largest gains on **small and medium objects**, consistent with qualitative boundary adherence improvements.

---

## ğŸ“˜ Citation

If you use SAMRI in your research, please cite:

```bibtex
@article{wang2025samri,
  title={SAMRI: Segment Anything Model for MRI},
  author={Wang, Zhao and Chandra, Shekhar and Dai, Wei and others},
  journal={Nature Communications},
  year={2025}
}
```

---

## ğŸ“„ License

This repository is released under the **Apache 2.0 License** (or specify otherwise).  
See the [LICENSE](LICENSE) file for details.

---

## ğŸ¤ Acknowledgments

Developed at **The University of Queensland (UQ)**,  
**School of Electrical Engineering and Computer Science (EECS)**.  

Special thanks to  
- **Dr. Shekhar â€œShakesâ€ Chandra** (Principal Supervisor)  
- **Dr. Wei Dai** (Co-supervisor)  
- **Bunya HPC Team** for infrastructure support  

Built upon **Meta AIâ€™s Segment Anything Model (SAM)**.  
We also thank open-source contributors and the MRI research community for dataset availability.

---

## ğŸ“¬ Contact

**Zhao Wang, Ph.D. Candidate**  
School of Electrical Engineering and Computer Science (EECS)  
The University of Queensland, Australia  
ğŸ“§ zhao.wang1@uq.edu.au
